# üöÄ Acc√©l√©ration GPU & Deep Learning - Guide Nexus33

Ce document d√©taille l'architecture et l'utilisation de l'acc√©l√©ration GPU pour Th3 Thirty3 (Nexus33).

## üìë Table des Mati√®res

1. [Architecture GPU](#1-architecture-gpu)
2. [Pr√©requis Syst√®me](#2-pr√©requis-syst√®me)
3. [D√©marrage Rapide](#3-d√©marrage-rapide)
4. [Docker & GPU Passthrough](#4-docker--gpu-passthrough)
5. [Entra√Ænement de Mod√®les (TensorFlow)](#5-entra√Ænement-de-mod√®les-tensorflow)
6. [Inf√©rence LLM (Ollama GPU)](#6-inf√©rence-llm-ollama-gpu)
7. [Monitoring & Debugging](#7-monitoring--debugging)

---

## 1. Architecture GPU

L'acc√©l√©ration GPU est utilis√©e par deux composants principaux :

| Composant | Technologie | Usage | Port Container |
|-----------|-------------|-------|----------------|
| **TensorFlow Trainer** | TensorFlow 2.x (CUDA 11/12) | Entra√Ænement de mod√®les classification/cyber | `5000` (API), `6006` (TensorBoard) |
| **Ollama** | Llama.cpp + CUDA | Inf√©rence LLM locale ultra-rapide | `11434` |

---

## 2. Pr√©requis Syst√®me

Pour que le GPU NVIDIA soit accessible via Docker sur Windows 11 :

- **GPU** : NVIDIA GeForce RTX 30/40 series recommand√© (VRAM >= 8GB)
- **Driver** : Dernier pilote NVIDIA Game Ready ou Studio
- **WSL2** : Version √† jour (`wsl --update`)
- **Docker Desktop** : Version 4.x+ avec support WSL2 activ√©

---

## 3. D√©marrage Rapide

Utilisez le raccourci **`Th3 Thirty3 GPU`** ou la commande suivante :

```bat
NEXUS33-docker.bat --gpu
```

Cela active le profil Docker `docker-compose.gpu.yml` qui monte le GPU dans les conteneurs.

---

## 4. Docker & GPU Passthrough

La configuration technique se trouve dans `docker-compose.gpu.yml`. 
Section critique pour le passthrough :

```yaml
deploy:
  resources:
    reservations:
      devices:
        - driver: nvidia
          count: 1
          capabilities: [gpu]
```

---

## 5. Entra√Ænement de Mod√®les (TensorFlow)

Le service `tensorflow-trainer` expose une API pour lancer des entra√Ænements.

### Lancer un entra√Ænement manuel :
Via l'interface Nexus33 ou directement :
```bash
curl -X POST http://localhost:5000/train \
  -H "Content-Type: application/json" \
  -d '{"dataset": "security_logs", "epochs": 10}'
```

---

## 6. Inf√©rence LLM (Ollama GPU)

Par d√©faut, Ollama tourne sur l'h√¥te pour maximiser les perfs, ou dans un conteneur d√©di√© si configur√©.
V√©rifiez l'utilisation GPU :
```powershell
nvidia-smi
```
Vous devriez voir un processus `ollama_llama_server.exe` utiliser de la VRAM lors des requ√™tes.

---

## 7. Monitoring & Debugging

**V√©rifier la visibilit√© du GPU :**
```bash
docker exec -it th3-gpu-trainer python -c "import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"
```

**Voir les logs TensorBoard :**
Acc√©dez √† `http://localhost:6006` pendant/apr√®s un entra√Ænement.
