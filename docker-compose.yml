# Th3 Thirty3 - Docker Compose Configuration
# Connects all services: Server, Frontend, and Training Dashboard

services:
  # Backend API Server
  server:
    build:
      context: ./server
      dockerfile: Dockerfile
    container_name: th3-server
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=production
      - PORT=3000
      # API Keys (loaded from .env or set here)
      - ANYTHING_LLM_URL=${ANYTHING_LLM_URL:-http://host.docker.internal:3001}
      - ANYTHING_LLM_KEY=${ANYTHING_LLM_KEY}
      - GROQ_API_KEY=${GROQ_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - PERPLEXITY_API_KEY=${PERPLEXITY_API_KEY}
      - HACKERGPT_TOKEN=${HACKERGPT_TOKEN}
      - OLLAMA_BASE_URL=http://host.docker.internal:11434
      - GPU_TRAINER_URL=http://tensorflow-trainer:5000
      - HEXSTRIKE_URL=http://hexstrike:8888
    volumes:
      - ./server/data:/app/data  # Persist model metrics and settings
      - ./server/.env:/app/.env:ro  # Mount environment file
      - ./server/credentials.json:/app/credentials.json  # Mount Google Credentials
    networks:
      - th3-network
    restart: unless-stopped
    depends_on:
      - hexstrike
    extra_hosts:
      - "host.docker.internal:host-gateway"  # Access host services (Ollama)

  # TensorFlow GPU Training Service
  tensorflow-trainer:
    build:
      context: ./gpu_training
      dockerfile: Dockerfile
    container_name: th3-gpu-trainer
    restart: unless-stopped
    ports:
      - "5000:5000" # Training API
      - "6006:6006" # TensorBoard
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - TF_FORCE_GPU_ALLOW_GROWTH=true
      - TF_GPU_ALLOCATOR=cuda_malloc_async
      - TRAINING_MODE=manual
      - OLLAMA_URL=http://host.docker.internal:11434
      - SERVER_URL=http://host.docker.internal:3000
    volumes:
      - ./gpu_training:/app
      - ./server/data:/app/server_data
      - gpu-models:/app/trained_models
      - gpu-cache:/root/.cache
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks:
      - th3-network

  # Frontend React App
  frontend:
    build:
      context: ./interface
      dockerfile: Dockerfile
    container_name: th3-frontend
    ports:
      - "5174:80"
    depends_on:
      - server
    networks:
      - th3-network
    restart: unless-stopped

  # HackerGPT Agent (Windows Compatible)
  hackergpt:
    image: node:lts
    container_name: th3-hackergpt
    command: npx @hackerai/local@latest --token ${HACKERGPT_TOKEN} --dangerous
    environment:
      - HACKERGPT_TOKEN=${HACKERGPT_TOKEN}
    networks:
      - th3-network
    restart: unless-stopped

  # HexStrike AI MCP Server (150+ Security Tools)
  hexstrike:
    build:
      context: ./hexstrike-ai
      dockerfile: Dockerfile
    container_name: th3-hexstrike
    ports:
      - "8888:8888"
    environment:
      - HEXSTRIKE_HOST=0.0.0.0
      - HEXSTRIKE_PORT=8888
    volumes:
      - hexstrike-data:/tmp/hexstrike_output
      - hexstrike-prowler:/tmp/prowler_output
    networks:
      - th3-network
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
    # Security: Run with limited privileges
    cap_add:
      - NET_RAW  # Required for nmap, masscan
      - NET_ADMIN  # Required for network scanning
    security_opt:
      - no-new-privileges:true

networks:
  th3-network:
    driver: bridge

volumes:
  server-data:
  hexstrike-data:
  hexstrike-prowler:
  gpu-models:
  gpu-cache:

