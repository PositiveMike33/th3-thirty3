# ğŸš€ Configuration Llama 3.2 Vision 11B - Guide Complet

## âœ… Installation TerminÃ©e

- âœ… Granite 3.1 MoE 1B **supprimÃ©**
- âœ… Llama 3.2 Vision 11B **installÃ©**
- âœ… Nomic Embed Text **dÃ©jÃ  prÃ©sent** (meilleur embedding)
- âœ… Gestionnaire automatique de mÃ©moire **crÃ©Ã©**

---

## ğŸ¯ Configuration AnythingLLM

### Ã‰tape 1 : Configurer le Workspace VPO

1. Ouvrir AnythingLLM : `http://localhost:3001`
2. Aller dans le workspace VPO :
   - `Expert Senior en Excellence OpÃ©rationnelle...`
3. Cliquer sur **âš™ï¸ Settings**

### Ã‰tape 2 : Configurer Ollama

Dans **Chat Settings** :

```
Provider: Ollama
Base URL: http://localhost:11434
Model: llama3.2-vision:11b
Temperature: 0.1
Max Tokens: 8192
```

### Ã‰tape 3 : Configurer l'Embedding

Dans **Vector Database** :

```
Provider: Ollama
Model: nomic-embed-text:latest
```

---

## ğŸ”§ Gestion Automatique de la MÃ©moire

Le systÃ¨me dÃ©charge automatiquement le modÃ¨le aprÃ¨s **5 minutes d'inactivitÃ©** pour libÃ©rer RAM/VRAM.

### Comment Ã§a fonctionne :

1. **Utilisation** â†’ ModÃ¨le chargÃ© en RAM/VRAM
2. **5 min d'inactivitÃ©** â†’ ModÃ¨le dÃ©chargÃ© automatiquement
3. **Nouvelle utilisation** â†’ ModÃ¨le rechargÃ© automatiquement

### Commandes Manuelles

```bash
# VÃ©rifier les modÃ¨les chargÃ©s
ollama ps

# DÃ©charger manuellement un modÃ¨le
ollama stop llama3.2-vision:11b

# Lister tous les modÃ¨les installÃ©s
ollama list
```

---

## ğŸ“Š Utilisation de la MÃ©moire

### Avant (avec Granite)
- **Granite 3.1 MoE 1B** : ~1.4 GB
- **Total** : ~1.4 GB

### AprÃ¨s (avec Llama 3.2 Vision)
- **Llama 3.2 Vision 11B** : ~7 GB (quand chargÃ©)
- **Nomic Embed** : ~274 MB (pour embeddings)
- **Total quand actif** : ~7.3 GB
- **Total quand inactif** : ~0 GB (dÃ©chargÃ© automatiquement)

---

## ğŸ¯ Utilisation

### Via AnythingLLM Workspace VPO

1. Ouvrir le workspace VPO
2. Envoyer une image + description d'incident
3. Le modÃ¨le se charge automatiquement
4. GÃ©nÃ¨re le rapport 5-Why
5. AprÃ¨s 5 min â†’ DÃ©chargement automatique

### Via API

```bash
# Le modÃ¨le se charge automatiquement Ã  la premiÃ¨re requÃªte
curl -X POST http://localhost:3000/incident/complete \
  -H "Content-Type: application/json" \
  -d '{
    "media": "data:image/jpeg;base64,...",
    "description": "Bourrage Star Wheel"
  }'
```

---

## ğŸ” VÃ©rification

### Tester le modÃ¨le

```bash
# Test simple
ollama run llama3.2-vision:11b "Bonjour, peux-tu m'aider?"

# Test avec image (exemple)
ollama run llama3.2-vision:11b "DÃ©cris cette image" < image.jpg
```

### VÃ©rifier l'embedding

```bash
ollama run nomic-embed-text "test embedding"
```

---

## ğŸ†˜ DÃ©pannage

### "Model not found"
```bash
# VÃ©rifier que le modÃ¨le est installÃ©
ollama list

# RÃ©installer si nÃ©cessaire
ollama pull llama3.2-vision:11b
```

### "Out of memory"
```bash
# DÃ©charger tous les modÃ¨les
ollama ps
ollama stop <model-name>

# Ou redÃ©marrer Ollama
# Windows : RedÃ©marrer le service Ollama
```

### ModÃ¨le ne se dÃ©charge pas automatiquement
```bash
# DÃ©charger manuellement
ollama stop llama3.2-vision:11b

# VÃ©rifier qu'aucun modÃ¨le n'est chargÃ©
ollama ps
```

---

## ğŸ“ ModÃ¨les InstallÃ©s

| ModÃ¨le | Taille | Usage | Auto-dÃ©charge |
|--------|--------|-------|---------------|
| **llama3.2-vision:11b** | 7.8 GB | Vision + 5-Why VPO | âœ… Oui (5 min) |
| **nomic-embed-text** | 274 MB | Embeddings (RAG) | âœ… Oui |

---

## ğŸ¯ Avantages de cette Configuration

1. âœ… **100% Local** : Aucune donnÃ©e ne sort de ton rÃ©seau
2. âœ… **100% Gratuit** : Pas de coÃ»t API
3. âœ… **Gestion MÃ©moire** : LibÃ©ration automatique RAM/VRAM
4. âœ… **Meilleur Vision Local** : Llama 3.2 Vision 11B
5. âœ… **Meilleur Embedding** : Nomic Embed Text
6. âœ… **Conforme AB InBev** : DonnÃ©es privÃ©es

---

## ğŸš€ Prochaines Ã‰tapes

1. âœ… Attendre fin du tÃ©lÃ©chargement
2. âœ… Configurer AnythingLLM (voir Ã‰tape 1-3 ci-dessus)
3. âœ… Tester avec un incident rÃ©el
4. âœ… VÃ©rifier le dÃ©chargement automatique aprÃ¨s 5 min

**C'est prÃªt ! ğŸ‰**
