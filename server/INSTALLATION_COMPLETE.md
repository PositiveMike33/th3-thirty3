# âœ… Installation ComplÃ¨te : Llama 3.2 Vision 11B

## ğŸ‰ RÃ©sumÃ©

### âœ… Actions EffectuÃ©es

1. **SupprimÃ©** : Granite 3.1 MoE 1B (~1.4 GB libÃ©rÃ©s)
2. **InstallÃ©** : Llama 3.2 Vision 11B (7.8 GB)
3. **ConservÃ©** : Nomic Embed Text (274 MB) - Meilleur embedding
4. **CrÃ©Ã©** : Gestionnaire automatique de mÃ©moire (`ollama_manager.js`)

### ğŸ“Š ModÃ¨les Actuels

```
NAME                       SIZE      USAGE
llama3.2-vision:11b        7.8 GB    Vision + 5-Why VPO
nomic-embed-text:latest    274 MB    Embeddings (RAG)
```

---

## ğŸ”§ Configuration AnythingLLM

### Pour le Workspace VPO

1. Ouvrir `http://localhost:3001`
2. Aller dans le workspace VPO
3. **Settings** â†’ **Chat Settings**

```
Provider: Ollama
Base URL: http://localhost:11434
Model: llama3.2-vision:11b
Temperature: 0.1
Max Tokens: 8192
```

4. **Settings** â†’ **Vector Database**

```
Provider: Ollama
Model: nomic-embed-text:latest
```

---

## ğŸ¯ Gestion Automatique de la MÃ©moire

### Comment Ã§a fonctionne

Le systÃ¨me dÃ©charge automatiquement le modÃ¨le aprÃ¨s **5 minutes d'inactivitÃ©** :

```
Utilisation â†’ ModÃ¨le chargÃ© (7.8 GB VRAM)
     â†“
5 min inactivitÃ©
     â†“
DÃ©chargement auto â†’ RAM/VRAM libÃ©rÃ©e (0 GB)
     â†“
Nouvelle utilisation â†’ Rechargement auto
```

### Commandes Manuelles

```bash
# Voir les modÃ¨les chargÃ©s en mÃ©moire
ollama ps

# DÃ©charger manuellement
ollama stop llama3.2-vision:11b

# VÃ©rifier qu'aucun modÃ¨le n'est chargÃ©
ollama ps  # Doit Ãªtre vide
```

---

## ğŸš€ Utilisation

### Via AnythingLLM

1. Ouvrir workspace VPO
2. Envoyer image + description incident
3. ModÃ¨le se charge automatiquement
4. GÃ©nÃ¨re rapport 5-Why
5. AprÃ¨s 5 min â†’ DÃ©chargement auto

### Via API

```bash
curl -X POST http://localhost:3000/incident/complete \
  -H "Content-Type: application/json" \
  -d '{
    "media": "data:image/jpeg;base64,...",
    "description": "Bourrage Star Wheel"
  }'
```

---

## ğŸ’¡ Avantages

1. âœ… **100% Local** : DonnÃ©es privÃ©es (conforme AB InBev)
2. âœ… **100% Gratuit** : Pas de coÃ»t API
3. âœ… **Gestion MÃ©moire** : LibÃ©ration auto RAM/VRAM
4. âœ… **Meilleur Vision Local** : Llama 3.2 Vision 11B
5. âœ… **Meilleur Embedding** : Nomic Embed Text
6. âœ… **Pas de limite** : RequÃªtes illimitÃ©es

---

## ğŸ“ Prochaines Ã‰tapes

1. âœ… Configurer AnythingLLM (voir ci-dessus)
2. âœ… Tester avec un incident rÃ©el
3. âœ… VÃ©rifier le dÃ©chargement auto aprÃ¨s 5 min

**Tout est prÃªt ! ğŸ‰**

---

## ğŸ“š Documentation

- **Guide complet** : `SETUP_LLAMA_VISION.md`
- **Gestionnaire mÃ©moire** : `ollama_manager.js`
- **Tests** : `test_incident_analysis.js`
