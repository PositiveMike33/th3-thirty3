# Th3 Thirty3 - GPU-Enabled Docker Compose
# TensorFlow + CUDA for Ethical Hacking AI Training

services:
  # ================================
  # TensorFlow GPU Training Service
  # ================================
  tensorflow-trainer:
    build:
      context: ./gpu_training
      dockerfile: Dockerfile
    container_name: th3-gpu-trainer
    restart: unless-stopped
    shm_size: '2gb'
    ports:
      - "5000:5000" # Training API
      - "6006:6006" # TensorBoard
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - TF_FORCE_GPU_ALLOW_GROWTH=true
      - TF_GPU_ALLOCATOR=cuda_malloc_async
      - TRAINING_MODE=manual
      - TF_CPP_MIN_LOG_LEVEL=2
      - OLLAMA_URL=http://host.docker.internal:11434
      - SERVER_URL=http://host.docker.internal:3000
      - GENERATION_MODEL=granite4:latest
      - EMBEDDING_MODEL=mxbai-embed-large:latest
    volumes:
      - ./gpu_training:/app
      - ./server/data:/app/server_data
      - gpu-models:/app/trained_models
      - gpu-cache:/root/.cache
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu, compute, utility ]
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks:
      - th3-network
    healthcheck:
      test: [ "CMD", "python", "-c", "import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))" ]
      interval: 30s
      timeout: 10s
      retries: 3

  # ================================
  # Backend API (with GPU support)
  # ================================
  server:
    build:
      context: ./server
      dockerfile: Dockerfile
    container_name: th3-server
    restart: unless-stopped
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=production
      - PORT=3000
      - GPU_TRAINER_URL=http://tensorflow-trainer:5000
      - OLLAMA_BASE_URL=http://host.docker.internal:11434
      - ANYTHING_LLM_URL=http://anythingllm:3001/api/v1
      - ANYTHING_LLM_KEY=${ANYTHING_LLM_KEY}
      - GROQ_API_KEY=${GROQ_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - GEMINI_API_KEY=${GEMINI_API_KEY}
    volumes:
      - ./server/data:/app/data
      - ./server/.env:/app/.env:ro
    depends_on:
      - tensorflow-trainer
      - anythingllm
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks:
      - th3-network

  # ================================
  # Frontend
  # ================================
  frontend:
    build:
      context: ./interface
      dockerfile: Dockerfile
    container_name: th3-frontend
    restart: unless-stopped
    ports:
      - "5174:80"
    depends_on:
      - server
    networks:
      - th3-network

  # ================================
  # AnythingLLM (RAG + Agents)
  # ================================
  anythingllm:
    image: mintplexlabs/anythingllm:latest
    container_name: th3-anythingllm
    restart: unless-stopped
    ports:
      - "3001:3001"
    environment:
      - STORAGE_DIR=/app/server/storage
      - JWT_SECRET=${JWT_SECRET:-th3-thirty3-secret}
      - LLM_PROVIDER=ollama
      - OLLAMA_BASE_PATH=http://host.docker.internal:11434
      - EMBEDDING_ENGINE=ollama
      - EMBEDDING_MODEL_PREF=nomic-embed-text:latest
      - VECTOR_DB=lancedb
      # AnythingLLM API Key for external access
      - AUTH_TOKEN=${ANYTHING_LLM_KEY}
    volumes:
      - anythingllm-storage:/app/server/storage
      - anythingllm-hotdir:/app/collector/hotdir
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks:
      - th3-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3001/api/ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ================================
  # Ollama with GPU (optional if running externally)
  # ================================
  ollama:
    image: ollama/ollama:latest
    container_name: th3-ollama-gpu
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    networks:
      - th3-network
    profiles:
      - with-ollama # Only start if explicitly requested

  # ================================
  # DartAI MCP Server
  # ================================
  dart-mcp:
    image: mcp/dart
    container_name: th3-dart-mcp
    restart: unless-stopped
    environment:
      - DART_TOKEN=${DART_TOKEN} # Set in .env file
    networks:
      - th3-network
    profiles:
      - with-dart # Only start if explicitly requested

networks:
  th3-network:
    driver: bridge

volumes:
  gpu-models:
  gpu-cache:
  ollama-data:
  anythingllm-storage:
  anythingllm-hotdir:
